{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "from torchvision import datasets, transforms\n",
    "from train import *\n",
    "from dataload import *\n",
    "from model import *\n",
    "#import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_is_initialized():\n",
    "    if dist.is_available():\n",
    "        if dist.is_initialized():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cpu')\n",
    "    print(device)\n",
    "    model = Net()\n",
    "    is_distributed = distributed_is_initialized()\n",
    "    print(\"is_distributed:\", is_distributed)\n",
    "    if is_distributed:\n",
    "        model.to(device)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    else:\n",
    "        #model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "    train_loader = MNISTDataLoader(args['root'], args['batch_size'], train=True, distributed=is_distributed)\n",
    "    test_loader = MNISTDataLoader(args['root'], args['batch_size'], train=False, distributed=is_distributed)\n",
    "\n",
    "    trainer = Trainer(model, optimizer, train_loader, test_loader, device)\n",
    "    trainer.fit(args['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    argv = {'world_size': int(2),\n",
    "            'rank': int(0),\n",
    "            'epochs': int(40),\n",
    "            'back_end': 'gloo',\n",
    "            'init_method': 'tcp://10.1.1.101:23456',\n",
    "            'lr': float(1e-3),\n",
    "            'root': 'data',\n",
    "            'batch_size': int(32)\n",
    "           }\n",
    "    \n",
    "    print(argv)\n",
    "    if argv['world_size'] > 1:\n",
    "        dist.init_process_group(\n",
    "            backend=argv['back_end'],\n",
    "            init_method=argv['init_method'],\n",
    "            world_size=argv['world_size'],\n",
    "            rank=argv['rank'],\n",
    "    )\n",
    "    print('Start Run')\n",
    "    run(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'world_size': 2, 'rank': 0, 'epochs': 40, 'back_end': 'nccl', 'init_method': 'tcp://10.1.1.101:23456', 'lr': 0.001, 'root': 'data', 'batch_size': 32}\n",
      "Start Run\n",
      "cuda\n",
      "is_distributed: True\n",
      "Epoch: 1/40, train loss: 0.610072, train acc: 77.30%, test loss: 0.455231, test acc: 82.95%.\n",
      "Epoch: 2/40, train loss: 0.397138, train acc: 85.36%, test loss: 0.394385, test acc: 85.62%.\n",
      "Epoch: 3/40, train loss: 0.346577, train acc: 86.93%, test loss: 0.360034, test acc: 86.95%.\n",
      "Epoch: 4/40, train loss: 0.315841, train acc: 88.18%, test loss: 0.342529, test acc: 87.59%.\n",
      "Epoch: 5/40, train loss: 0.294255, train acc: 88.98%, test loss: 0.321948, test acc: 88.41%.\n",
      "Epoch: 6/40, train loss: 0.277202, train acc: 89.55%, test loss: 0.310230, test acc: 88.48%.\n",
      "Epoch: 7/40, train loss: 0.262937, train acc: 90.02%, test loss: 0.309009, test acc: 88.73%.\n",
      "Epoch: 8/40, train loss: 0.252971, train acc: 90.48%, test loss: 0.305233, test acc: 88.93%.\n",
      "Epoch: 9/40, train loss: 0.243077, train acc: 90.80%, test loss: 0.303383, test acc: 88.93%.\n",
      "Epoch: 10/40, train loss: 0.233526, train acc: 91.08%, test loss: 0.303143, test acc: 89.22%.\n",
      "Epoch: 11/40, train loss: 0.223551, train acc: 91.60%, test loss: 0.308150, test acc: 89.19%.\n",
      "Epoch: 12/40, train loss: 0.215551, train acc: 91.88%, test loss: 0.315971, test acc: 89.30%.\n",
      "Epoch: 13/40, train loss: 0.207464, train acc: 92.21%, test loss: 0.319048, test acc: 89.36%.\n",
      "Epoch: 14/40, train loss: 0.200133, train acc: 92.49%, test loss: 0.324082, test acc: 89.28%.\n",
      "Epoch: 15/40, train loss: 0.194543, train acc: 92.64%, test loss: 0.323585, test acc: 89.45%.\n",
      "Epoch: 16/40, train loss: 0.187709, train acc: 92.96%, test loss: 0.334334, test acc: 89.61%.\n",
      "Epoch: 17/40, train loss: 0.182468, train acc: 93.12%, test loss: 0.341971, test acc: 89.50%.\n",
      "Epoch: 18/40, train loss: 0.176499, train acc: 93.44%, test loss: 0.342840, test acc: 89.61%.\n",
      "Epoch: 19/40, train loss: 0.170633, train acc: 93.71%, test loss: 0.350792, test acc: 89.80%.\n",
      "Epoch: 20/40, train loss: 0.165936, train acc: 93.77%, test loss: 0.342653, test acc: 89.67%.\n",
      "Epoch: 21/40, train loss: 0.159139, train acc: 94.02%, test loss: 0.360684, test acc: 89.49%.\n",
      "Epoch: 22/40, train loss: 0.155588, train acc: 94.13%, test loss: 0.374220, test acc: 89.30%.\n",
      "Epoch: 23/40, train loss: 0.153445, train acc: 94.15%, test loss: 0.396234, test acc: 88.83%.\n",
      "Epoch: 24/40, train loss: 0.146938, train acc: 94.36%, test loss: 0.385482, test acc: 89.25%.\n",
      "Epoch: 25/40, train loss: 0.143330, train acc: 94.39%, test loss: 0.410111, test acc: 88.97%.\n",
      "Epoch: 26/40, train loss: 0.138900, train acc: 94.73%, test loss: 0.407597, test acc: 89.14%.\n",
      "Epoch: 27/40, train loss: 0.135211, train acc: 94.91%, test loss: 0.409645, test acc: 89.20%.\n",
      "Epoch: 28/40, train loss: 0.129715, train acc: 95.04%, test loss: 0.431714, test acc: 89.14%.\n",
      "Epoch: 29/40, train loss: 0.126304, train acc: 95.21%, test loss: 0.432154, test acc: 89.40%.\n",
      "Epoch: 30/40, train loss: 0.124857, train acc: 95.23%, test loss: 0.464656, test acc: 88.79%.\n",
      "Epoch: 31/40, train loss: 0.120715, train acc: 95.38%, test loss: 0.464980, test acc: 88.85%.\n",
      "Epoch: 32/40, train loss: 0.119369, train acc: 95.46%, test loss: 0.462602, test acc: 88.98%.\n",
      "Epoch: 33/40, train loss: 0.114215, train acc: 95.51%, test loss: 0.510118, test acc: 88.55%.\n",
      "Epoch: 34/40, train loss: 0.111284, train acc: 95.81%, test loss: 0.494742, test acc: 89.17%.\n",
      "Epoch: 35/40, train loss: 0.109902, train acc: 95.77%, test loss: 0.533914, test acc: 88.83%.\n",
      "Epoch: 36/40, train loss: 0.114109, train acc: 95.55%, test loss: 0.523993, test acc: 88.34%.\n",
      "Epoch: 37/40, train loss: 0.102806, train acc: 96.03%, test loss: 0.533533, test acc: 88.89%.\n",
      "Epoch: 38/40, train loss: 0.101451, train acc: 96.09%, test loss: 0.585063, test acc: 88.41%.\n",
      "Epoch: 39/40, train loss: 0.099937, train acc: 96.27%, test loss: 0.554124, test acc: 88.64%.\n",
      "Epoch: 40/40, train loss: 0.094556, train acc: 96.31%, test loss: 0.574302, test acc: 88.98%.\n"
     ]
    }
   ],
   "source": [
    "#os.environ['MASTER_ADDR'] = 'localhost'\n",
    "#os.environ['MASTER_PORT'] = '23456'\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
