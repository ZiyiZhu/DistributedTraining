{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "from torchvision import datasets, transforms\n",
    "from train import *\n",
    "from dataload import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_is_initialized():\n",
    "    if dist.is_available():\n",
    "        if dist.is_initialized():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model = Net()\n",
    "    is_distributed = distributed_is_initialized()\n",
    "    print(\"is_distributed:\", is_distributed)\n",
    "    if is_distributed:\n",
    "        model.to(device)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    else:\n",
    "        #model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "\n",
    "    train_loader = MNISTDataLoader(args['root'], args['batch_size'], train=True, distributed=is_distributed)\n",
    "    test_loader = MNISTDataLoader(args['root'], args['batch_size'], train=False, distributed=is_distributed)\n",
    "\n",
    "    trainer = Trainer(model, optimizer, train_loader, test_loader, device)\n",
    "    trainer.fit(args['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    argv = {'world_size': int(2),\n",
    "            'rank': int(0),\n",
    "            'epochs': int(10),\n",
    "            'back_end': 'nccl',\n",
    "            'init_method': 'tcp://10.1.1.101:23456',\n",
    "            'lr': float(1e-3),\n",
    "            'root': 'data',\n",
    "            'batch_size': int(32)\n",
    "           }\n",
    "    \n",
    "    print(argv)\n",
    "    if argv['world_size'] > 1:\n",
    "        dist.init_process_group(\n",
    "            backend=argv['back_end'],\n",
    "            init_method=argv['init_method'],\n",
    "            world_size=argv['world_size'],\n",
    "            rank=argv['rank'],\n",
    "    )\n",
    "    print('Start Run')\n",
    "    run(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
